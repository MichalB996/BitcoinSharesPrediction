{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlesearch\n",
    "from newspaper import Article, news_pool\n",
    "from datetime import timedelta, date, datetime\n",
    "import arrow\n",
    "from newspaper import Config\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from langdetect import detect, detect_langs\n",
    "from date_guesser import guess_date, Accuracy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_google_urls(query, quantity, startdate, enddate):\n",
    "    url_list = []\n",
    "    pre_rejected_list = []\n",
    "    current_date = startdate.datetime\n",
    "    while current_date <= enddate:\n",
    "        my_tbs = googlesearch.get_tbs(current_date, current_date)\n",
    "        # for url in googlesearch.search_news(query, tld=\"com\", tpe='news', stop=None, pause=2, lang='en', tbs=my_tbs):\n",
    "        pause = random.randrange(15, 60)\n",
    "        counter = 0\n",
    "        for url in googlesearch.search(query, tld=\"com\", tpe='nws', num=50, stop=300, pause=pause, lang='en',\n",
    "                                       tbs=my_tbs):\n",
    "            text = url.rstrip('/').split('/')[-1]\n",
    "            text = re.sub(r'\\W+', ' ', text).replace('html', '')\n",
    "            if text is None or len(text.split()) <= 4:\n",
    "                url_list.append(url)\n",
    "            else:\n",
    "                try:\n",
    "                    detected_langs = detect_langs(text)\n",
    "                except:\n",
    "                    detected_langs = None\n",
    "\n",
    "                langs = [x.lang for x in detected_langs]\n",
    "                if ('en' not in langs) and len(langs) == 1:\n",
    "                    print(\"pre_rejected: \" + ' '.join(\n",
    "                        [x.lang + '%.2f' % x.prob + ' ' for x in detected_langs]) + \"   \" + url)\n",
    "                    print(\"\\t\" + text)\n",
    "                    pre_rejected_list.append(url)\n",
    "                else:\n",
    "                    url_list.append(url)\n",
    "\n",
    "        yield url_list, current_date\n",
    "        with open(os.getcwd() + \"\\\\\" + directory_name + \"\\\\\" + query_name + \"_pre_rejected.txt\", \"a+\",\n",
    "                  encoding=\"utf-8\") as f:\n",
    "            for pre_rejected in pre_rejected_list:\n",
    "                f.write(pre_rejected)\n",
    "                f.write('\\n')\n",
    "\n",
    "        current_date += timedelta(days=1)\n",
    "        url_list.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2016, 12, 6)\n",
    "end_date = date(2020, 4, 26)\n",
    "\n",
    "directory_name = 'Articles'\n",
    "try:\n",
    "    os.mkdir(os.getcwd() + \"\\\\\" + directory_name)\n",
    "except:\n",
    "    print('Warning: Article directory already exists.')\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36'\n",
    "config_browser = Config()\n",
    "config_browser.browser_user_agent = user_agent\n",
    "\n",
    "query = \"Bitcoin\"\n",
    "query_name = query.split(' ')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for urlList, current_date in Get_google_urls(query=query, quantity=1000, startdate=arrow.get(datetime(2016, 12, 6)),\n",
    "                                             enddate=arrow.get(datetime(2020, 4, 26))):\n",
    "    for http in urlList:\n",
    "        article = Article(http, config=config_browser)\n",
    "        try:\n",
    "            article.download()\n",
    "        except:\n",
    "            print('Not parsed:' + http)\n",
    "\n",
    "        if article.download_exception_msg != None:\n",
    "            print(\"DOWNLOAD EXCEPTION: \" + http)\n",
    "            print(article.download_exception_msg)\n",
    "        else:\n",
    "            try:\n",
    "                article.parse()\n",
    "            except:\n",
    "                print('Not parsed:' + http)\n",
    "            title = 'None'\n",
    "            title_stripped = 'None'\n",
    "\n",
    "            # Tylko informacyjnie dla porównania. Data google wydaje się być lepszym rozwiązaniem.\n",
    "            publish_date = article.extractor.get_publishing_date(url='', doc=article.clean_doc)\n",
    "            if publish_date is not None:\n",
    "                article_date = arrow.get(publish_date)\n",
    "                if article_date.date() != current_date.date():\n",
    "                    with open(os.getcwd() + \"\\\\\" + directory_name + \"\\\\\" + query_name + \"_wrong_date.txt\", \"a+\",\n",
    "                              encoding=\"utf-8\") as f:\n",
    "                        print(\"wrong_date: \" + http)\n",
    "                        f.write(http)\n",
    "                        f.write('\\n')\n",
    "                        f.write(\"GOOGLE: \" + str(current_date.date()) + \"  ARTICLE: \" + str(article_date))\n",
    "                        f.write('\\n')\n",
    "            # else:\n",
    "            #     article_date = None\n",
    "\n",
    "            article_date = current_date\n",
    "\n",
    "            try:\n",
    "                if len(article.text) > 500 and detect(article.text) == 'en' and article_date is not None:\n",
    "                    if hasattr(article, 'title'):\n",
    "                        title = article.title\n",
    "                        title_stripped = ''.join(e for e in title[:50] if e.isalnum() or e in \" $%\")\n",
    "\n",
    "                    directory = os.getcwd() + \"\\\\\" + directory_name + \"\\\\\" + article_date.strftime(\n",
    "                        \"%d.%m.%Y\") + \"\\\\\" + query_name + \"\\\\\"\n",
    "                    filename = directory + title_stripped + \".txt\"\n",
    "                    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(\"DATE:  \" + article_date.strftime(\"%d.%m.%Y, %H:%M:%S\"))\n",
    "                        f.write('\\n')\n",
    "                        f.write(\"TITLE: \" + title)\n",
    "                        f.write('\\n')\n",
    "                        f.write(\"URL:   \" + http)\n",
    "                        f.write('\\n\\n')\n",
    "                        f.write(article.text)\n",
    "                else:\n",
    "                    with open(os.getcwd() + \"\\\\\" + directory_name + \"\\\\\" + query_name + \"_rejected.txt\", \"a+\",\n",
    "                              encoding=\"utf-8\") as f:\n",
    "                        print(\"rejected: \" + http)\n",
    "                        f.write(http)\n",
    "                        f.write('\\n')\n",
    "            except:\n",
    "                print(\"exception: \" +\n",
    "                      article_date.strftime(\"%d.%m.%Y, %H:%M:%S\") + \"  \" + http)\n",
    "                with open(os.getcwd() + \"\\\\\" + directory_name + \"\\\\\" + query_name + \"_exception.txt\", \"a+\",\n",
    "                          encoding=\"utf-8\") as f:\n",
    "                    f.write(http)\n",
    "                    f.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
